# Training Configuration for F1 Conversational AI

# Model Configuration
model:
  base_model: "google/gemma-7b" # or "meta-llama/Llama-2-7b-hf"
  model_type: "causal_lm"
  max_length: 2048
  padding: "max_length"
  truncation: true

# Training Parameters
training:
  output_dir: "models/checkpoints"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 5e-5
  weight_decay: 0.01
  warmup_steps: 100
  logging_steps: 10
  eval_steps: 500
  save_steps: 1000
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  fp16: true
  dataloader_num_workers: 4
  remove_unused_columns: false

# LoRA Configuration
lora:
  r: 16
  lora_alpha: 32
  target_modules:
    [
      "q_proj",
      "v_proj",
      "k_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj",
    ]
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"

# Data Configuration
data:
  train_file: "data/curated/train.jsonl"
  eval_file: "data/curated/eval.jsonl"
  test_file: "data/curated/test.jsonl"
  max_train_samples: null # Set to number for debugging
  max_eval_samples: null
  max_test_samples: null

# Evaluation Configuration
evaluation:
  eval_strategy: "steps"
  eval_steps: 500
  prediction_loss_only: false
  include_inputs_for_metrics: false

# Logging and Monitoring
logging:
  logging_dir: "logs"
  logging_strategy: "steps"
  logging_steps: 10
  logging_first_step: true
  logging_nan_inf_filter: true

# Weights & Biases
wandb:
  project: "f1-conversational-ai"
  entity: "your-entity"
  tags: ["f1", "llm", "fine-tuning"]
  notes: "F1 Conversational AI Fine-tuning Experiment"

# Hardware Configuration
hardware:
  use_cuda: true
  cuda_visible_devices: "0"
  mixed_precision: "fp16"
  dataloader_pin_memory: true
  dataloader_num_workers: 4

# Paths
paths:
  data_dir: "data"
  model_dir: "models"
  output_dir: "results"
  cache_dir: ".cache"
  temp_dir: "temp"

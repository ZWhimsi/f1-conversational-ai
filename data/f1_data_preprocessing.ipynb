{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1.3: F1 Data Preprocessing and Cleaning\n",
    "\n",
    "This notebook covers the tasks for preprocessing and cleaning the raw data collected in the previous steps. The goal is to create a clean, structured, and consistent dataset ready for analysis and model training.\n",
    "\n",
    "**Tasks:**\n",
    "1.  **Load Raw Data:** Ingest all datasets from the `f1_data` directory.\n",
    "2.  **Standard Text Cleaning:** Apply procedures like lowercasing, removing HTML tags, and stripping special characters from text fields.\n",
    "3.  **Handle Missing Data:** Inspect each dataset for null values and apply an appropriate strategy (e.g., filling or dropping).\n",
    "4.  **Standardize Schema:** Ensure data types are correct (e.g., dates, numbers) across all datasets.\n",
    "5.  **Save Cleaned Data:** Store the processed dataframes in a versioned directory (`cleaned_data_v1.0`) in the efficient Parquet format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Configuration\n",
    "\n",
    "Import necessary libraries and define the file paths for our raw data and the output directory for the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Scraped Data Path: f1_data\\scraped_data\n",
      "Raw Kaggle Data Path: f1_data\\kaggle_data\n",
      "Cleaned Data Output Path: f1_data\\cleaned_data_v1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input paths based on the project structure\n",
    "SCRAPED_DATA_PATH = os.path.join('f1_data', 'scraped_data')\n",
    "KAGGLE_DATA_PATH = os.path.join('f1_data', 'kaggle_data')\n",
    "\n",
    "# Output path for the cleaned, versioned dataset\n",
    "CLEANED_DATA_PATH = os.path.join('f1_data', 'cleaned_data_v1.0')\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(CLEANED_DATA_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Raw Scraped Data Path: {SCRAPED_DATA_PATH}\")\n",
    "print(f\"Raw Kaggle Data Path: {KAGGLE_DATA_PATH}\")\n",
    "print(f\"Cleaned Data Output Path: {CLEANED_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Cleaning Utility Function\n",
    "\n",
    "This function will be our general-purpose tool for cleaning text data. It removes HTML tags, converts text to lowercase, and strips out special characters and extra whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies standard text cleaning procedures.\n",
    "    - Removes HTML tags.\n",
    "    - Converts text to lowercase.\n",
    "    - Removes special characters, keeping alphanumeric and basic punctuation.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove HTML tags using BeautifulSoup\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters (keeping letters, numbers, and basic punctuation)\n",
    "    text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Process Scraped News Articles\n",
    "\n",
    "Load the `f1_news_articles.csv`, apply the text cleaning function to the `title` and `summary` columns, and check for any missing values or duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scraped_news():\n",
    "    \"\"\"\n",
    "    Loads, cleans, and saves the scraped news articles.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Processing Scraped News Articles ---\")\n",
    "    news_file = os.path.join(SCRAPED_DATA_PATH, 'f1_news_articles.csv')\n",
    "\n",
    "    try:\n",
    "        news_df = pd.read_csv(news_file)\n",
    "        print(\"Original news data:\")\n",
    "        print(news_df.head())\n",
    "        print(f\"\\nShape: {news_df.shape}\")\n",
    "        print(f\"\\nMissing values:\\n{news_df.isnull().sum()}\")\n",
    "\n",
    "        # Clean text columns\n",
    "        news_df['title_cleaned'] = news_df['title'].apply(clean_text)\n",
    "        news_df['summary_cleaned'] = news_df['summary'].apply(clean_text)\n",
    "\n",
    "        # Handle missing summaries by filling with an empty string\n",
    "        news_df['summary_cleaned'] = news_df['summary_cleaned'].fillna('')\n",
    "        \n",
    "        # Drop original text columns and duplicates\n",
    "        cleaned_news_df = news_df[['source', 'title_cleaned', 'summary_cleaned', 'link']].copy()\n",
    "        cleaned_news_df.drop_duplicates(subset=['link'], inplace=True)\n",
    "        cleaned_news_df.rename(columns={'title_cleaned': 'title', 'summary_cleaned': 'summary'}, inplace=True)\n",
    "        \n",
    "        print(\"\\nCleaned news data:\")\n",
    "        print(cleaned_news_df.head())\n",
    "        print(f\"\\nShape after cleaning: {cleaned_news_df.shape}\")\n",
    "\n",
    "        # Save to CSV instead of Parquet to avoid dependency issues\n",
    "        output_file = os.path.join(CLEANED_DATA_PATH, 'cleaned_news_articles.csv')\n",
    "        cleaned_news_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nSuccessfully cleaned and saved news articles to {output_file}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: News articles file not found at {news_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Scraped News Articles ---\n",
      "Original news data:\n",
      "                source                                              title  \\\n",
      "0  Motorsport Magazine  Verstappen nears historic F1 comeback - US GP ...   \n",
      "1  Motorsport Magazine  Verstappen's saving F1 from papaya coporate pr...   \n",
      "2  Motorsport Magazine  Apple's F1 gamble gets mixed response - What y...   \n",
      "3  Motorsport Magazine  Mark Hughes: How Verstappen crushed McLaren af...   \n",
      "4  Motorsport Magazine  When is the next F1 race? Full calendar for 20...   \n",
      "\n",
      "                                             summary  \\\n",
      "0  Verstappen's double win in Austin moved him cl...   \n",
      "1  Could F1 2025 be about to wake from its slumbe...   \n",
      "2  Cadillac's quiet confidence, Apple's broadcast...   \n",
      "3  Behind Max Verstappenâ€™s perfect 2025 United St...   \n",
      "4  Full F1 schedule for the year, including the n...   \n",
      "\n",
      "                                                link  \n",
      "0  https://www.motorsportmagazine.com/articles/si...  \n",
      "1  https://www.motorsportmagazine.com/articles/si...  \n",
      "2  https://www.motorsportmagazine.com/articles/si...  \n",
      "3  https://www.motorsportmagazine.com/articles/si...  \n",
      "4  https://www.motorsportmagazine.com/articles/si...  \n",
      "\n",
      "Shape: (512, 4)\n",
      "\n",
      "Missing values:\n",
      "source     0\n",
      "title      0\n",
      "summary    0\n",
      "link       0\n",
      "dtype: int64\n",
      "\n",
      "Cleaned news data:\n",
      "                source                                              title  \\\n",
      "0  Motorsport Magazine  verstappen nears historic f1 comeback - us gp ...   \n",
      "1  Motorsport Magazine  verstappens saving f1 from papaya coporate pro...   \n",
      "2  Motorsport Magazine  apples f1 gamble gets mixed response - what yo...   \n",
      "3  Motorsport Magazine  mark hughes how verstappen crushed mclaren aft...   \n",
      "4  Motorsport Magazine  when is the next f1 race? full calendar for 20...   \n",
      "\n",
      "                                             summary  \\\n",
      "0  verstappens double win in austin moved him clo...   \n",
      "1  could f1 2025 be about to wake from its slumbe...   \n",
      "2  cadillacs quiet confidence, apples broadcastin...   \n",
      "3  behind max verstappens perfect 2025 united sta...   \n",
      "4  full f1 schedule for the year, including the n...   \n",
      "\n",
      "                                                link  \n",
      "0  https://www.motorsportmagazine.com/articles/si...  \n",
      "1  https://www.motorsportmagazine.com/articles/si...  \n",
      "2  https://www.motorsportmagazine.com/articles/si...  \n",
      "3  https://www.motorsportmagazine.com/articles/si...  \n",
      "4  https://www.motorsportmagazine.com/articles/si...  \n",
      "\n",
      "Shape after cleaning: (512, 4)\n",
      "\n",
      "Successfully cleaned and saved news articles to f1_data\\cleaned_data_v1.0\\cleaned_news_articles.csv\n"
     ]
    }
   ],
   "source": [
    "process_scraped_news()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Process Scraped Autosport Race Results\n",
    "\n",
    "Here, we'll find all the individual race result CSVs, combine them into a single DataFrame, standardize the data, and handle any inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scraped_results():\n",
    "    \"\"\"\n",
    "    Loads, cleans, and saves scraped race results.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Processing Scraped Race Results ---\")\n",
    "    results_files = glob.glob(os.path.join(SCRAPED_DATA_PATH, 'autosport_f1_results_*.csv'))\n",
    "\n",
    "    if results_files:\n",
    "        all_results_df = pd.concat([pd.read_csv(f) for f in results_files], ignore_index=True)\n",
    "        \n",
    "        print(\"Original combined results data:\")\n",
    "        print(all_results_df.head())\n",
    "        print(f\"\\nShape: {all_results_df.shape}\")\n",
    "        print(f\"\\nMissing values:\\n{all_results_df.isnull().sum()}\")\n",
    "\n",
    "        # Basic cleaning and standardization\n",
    "        all_results_df['Driver'] = all_results_df['Driver'].apply(clean_text)\n",
    "        all_results_df['Team'] = all_results_df['Team'].apply(clean_text)\n",
    "        \n",
    "        # Convert 'Pos' to numeric, coercing errors (like 'NC') to NaN\n",
    "        all_results_df['Pos'] = pd.to_numeric(all_results_df['Pos'], errors='coerce')\n",
    "        \n",
    "        # Fill NaN in 'Time' with a placeholder\n",
    "        all_results_df['Time'] = all_results_df['Time'].fillna('Not Classified')\n",
    "        \n",
    "        # Drop duplicates if any\n",
    "        all_results_df.drop_duplicates(inplace=True)\n",
    "\n",
    "        print(\"\\nCleaned race results data:\")\n",
    "        print(all_results_df.head())\n",
    "        print(f\"\\nShape after cleaning: {all_results_df.shape}\")\n",
    "\n",
    "        # Save to CSV\n",
    "        output_file = os.path.join(CLEANED_DATA_PATH, 'cleaned_race_results.csv')\n",
    "        all_results_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nSuccessfully cleaned and saved race results to {output_file}\")\n",
    "    else:\n",
    "        print(\"No scraped race result files found to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Scraped Race Results ---\n",
      "No scraped race result files found to process.\n"
     ]
    }
   ],
   "source": [
    "process_scraped_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Process Kaggle Datasets\n",
    "\n",
    "Now we'll process each of the Kaggle datasets. Since they are more structured, the focus will be on data type consistency and handling any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_kaggle_datasets():\n",
    "    \"\"\"\n",
    "    Wrapper function to process all Kaggle datasets.\n",
    "    \"\"\"\n",
    "    def process_file(file_path, output_name, text_column=None, date_column=None):\n",
    "        \"\"\"A helper function to load, clean, and save a Kaggle CSV.\"\"\"\n",
    "        print(f\"\\n--- Processing {output_name} ---\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8', on_bad_lines='warn')\n",
    "            print(f\"Original shape: {df.shape}\")\n",
    "            \n",
    "            # Clean text column if specified\n",
    "            if text_column and text_column in df.columns:\n",
    "                df[text_column] = df[text_column].apply(clean_text)\n",
    "            \n",
    "            # Convert date column if specified\n",
    "            if date_column and date_column in df.columns:\n",
    "                df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
    "                \n",
    "            # Drop rows where critical info might be missing after conversion\n",
    "            df.dropna(subset=[c for c in [text_column, date_column] if c], inplace=True)\n",
    "            \n",
    "            print(f\"Shape after cleaning: {df.shape}\")\n",
    "            \n",
    "            # Save to CSV\n",
    "            output_file = os.path.join(CLEANED_DATA_PATH, f'cleaned_{output_name}.csv')\n",
    "            df.to_csv(output_file, index=False)\n",
    "            print(f\"Successfully saved cleaned data to {output_file}\")\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found at {file_path}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Process each Kaggle dataset\n",
    "    process_file(\n",
    "        os.path.join(KAGGLE_DATA_PATH, 'trending_tweets', 'F1_tweets.csv'), \n",
    "        'tweets', \n",
    "        text_column='text', \n",
    "        date_column='date'\n",
    "    )\n",
    "\n",
    "    process_file(\n",
    "        os.path.join(KAGGLE_DATA_PATH, 'reddit_comments', 'kaggle_RC_2019-05.csv'),\n",
    "        'reddit_comments',\n",
    "        text_column='body'\n",
    "    )\n",
    "\n",
    "    process_file(\n",
    "        os.path.join(KAGGLE_DATA_PATH, 'fan_ratings', 'aggregated_kaggle.csv'),\n",
    "        'fan_ratings',\n",
    "        date_column='Y'\n",
    "    )\n",
    "\n",
    "    history_files = {\n",
    "        'constructors_performance': 'Constructor_Performance.csv',\n",
    "        'constructor_rankings': 'Constructor_Rankings.csv',\n",
    "        'drivers_details': 'Driver_Details.csv',\n",
    "        'driver_rankings': 'Driver_Rankings.csv',\n",
    "        'lap_times': 'Lap_Timings.csv',\n",
    "        'pit_stop_records': 'Pit_Stop_Records.csv',\n",
    "        'qualifying_results': 'Qualifying_Results.csv',\n",
    "        'race_results': 'Race_Results.csv',\n",
    "        'race_schedule': 'Race_Schedule.csv',\n",
    "        'race_status': 'Race_Status.csv',\n",
    "        'seasonal_summary': 'Season_Summaries.csv',\n",
    "        'sprint_results': 'Sprint_Race_Results.csv',\n",
    "        'team_details': 'Team_Details.csv',\n",
    "        'track_information': 'Track_Information.csv'\n",
    "\n",
    "    }\n",
    "\n",
    "    for name, filename in history_files.items():\n",
    "        process_file(\n",
    "            os.path.join(KAGGLE_DATA_PATH, 'championship_history', filename),\n",
    "            name\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing tweets ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zengq\\AppData\\Local\\Temp\\ipykernel_29144\\1526293175.py:9: DtypeWarning: Columns (5,6,7,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8', on_bad_lines='warn')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (632388, 13)\n",
      "Shape after cleaning: (632384, 13)\n",
      "Successfully saved cleaned data to f1_data\\cleaned_data_v1.0\\cleaned_tweets.csv\n",
      "\n",
      "--- Processing reddit_comments ---\n",
      "Original shape: (1000000, 4)\n",
      "Shape after cleaning: (1000000, 4)\n",
      "Successfully saved cleaned data to f1_data\\cleaned_data_v1.0\\cleaned_reddit_comments.csv\n",
      "\n",
      "--- Processing fan_ratings ---\n",
      "Original shape: (202, 7)\n",
      "Shape after cleaning: (202, 7)\n",
      "Successfully saved cleaned data to f1_data\\cleaned_data_v1.0\\cleaned_fan_ratings.csv\n",
      "\n",
      "--- Processing constructors_performance ---\n",
      "Original shape: (12505, 5)\n",
      "Shape after cleaning: (12505, 5)\n",
      "Successfully saved cleaned data to f1_data\\cleaned_data_v1.0\\cleaned_constructors_performance.csv\n",
      "\n",
      "--- Processing constructor_rankings ---\n",
      "Original shape: (13271, 7)\n",
      "Shape after cleaning: (13271, 7)\n",
      "Successfully saved cleaned data to f1_data\\cleaned_data_v1.0\\cleaned_constructor_rankings.csv\n",
      "\n",
      "--- Processing drivers_details ---\n",
      "Original shape: (859, 9)\n",
      "Shape after cleaning: (859, 9)\n",
      "Successfully saved cleaned data to f1_data\\cleaned_data_v1.0\\cleaned_drivers_details.csv\n",
      "\n",
      "--- Processing driver_rankings ---\n",
      "Original shape: (34595, 7)\n",
      "Shape after cleaning: (34595, 7)\n",
      "Successfully saved cleaned data to f1_data\\cleaned_data_v1.0\\cleaned_driver_rankings.csv\n",
      "\n",
      "--- Processing lap_times ---\n",
      "Original shape: (575029, 6)\n",
      "Shape after cleaning: (575029, 6)\n",
      "Successfully saved cleaned data to f1_data\\cleaned_data_v1.0\\cleaned_lap_times.csv\n",
      "\n",
      "--- Processing pit_stop_records ---\n",
      "Original shape: (10990, 7)\n",
      "Shape after cleaning: (10990, 7)\n",
      "Successfully saved cleaned data to f1_data\\cleaned_data_v1.0\\cleaned_pit_stop_records.csv\n",
      "\n",
      "--- Processing qualifying_results ---\n",
      "Original shape: (10254, 9)\n",
      "Shape after cleaning: (10254, 9)\n",
      "Successfully saved cleaned data to f1_data\\cleaned_data_v1.0\\cleaned_qualifying_results.csv\n",
      "\n",
      "--- Processing race_results ---\n",
      "Original shape: (26519, 18)\n",
      "Shape after cleaning: (26519, 18)\n",
      "Successfully saved cleaned data to f1_data\\cleaned_data_v1.0\\cleaned_race_results.csv\n",
      "\n",
      "--- Processing race_schedule ---\n",
      "Original shape: (1125, 18)\n",
      "Shape after cleaning: (1125, 18)\n",
      "Successfully saved cleaned data to f1_data\\cleaned_data_v1.0\\cleaned_race_schedule.csv\n",
      "\n",
      "--- Processing race_status ---\n",
      "Original shape: (139, 2)\n",
      "Shape after cleaning: (139, 2)\n",
      "Successfully saved cleaned data to f1_data\\cleaned_data_v1.0\\cleaned_race_status.csv\n",
      "\n",
      "--- Processing seasonal_summary ---\n",
      "Original shape: (75, 2)\n",
      "Shape after cleaning: (75, 2)\n",
      "Successfully saved cleaned data to f1_data\\cleaned_data_v1.0\\cleaned_seasonal_summary.csv\n",
      "\n",
      "--- Processing sprint_results ---\n",
      "Original shape: (300, 16)\n",
      "Shape after cleaning: (300, 16)\n",
      "Successfully saved cleaned data to f1_data\\cleaned_data_v1.0\\cleaned_sprint_results.csv\n",
      "\n",
      "--- Processing team_details ---\n",
      "Original shape: (212, 5)\n",
      "Shape after cleaning: (212, 5)\n",
      "Successfully saved cleaned data to f1_data\\cleaned_data_v1.0\\cleaned_team_details.csv\n",
      "\n",
      "--- Processing track_information ---\n",
      "Original shape: (77, 9)\n",
      "Shape after cleaning: (77, 9)\n",
      "Successfully saved cleaned data to f1_data\\cleaned_data_v1.0\\cleaned_track_information.csv\n"
     ]
    }
   ],
   "source": [
    "process_kaggle_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion\n",
    "\n",
    "All raw datasets have been processed, cleaned, and saved to the `f1_data/cleaned_data_v1.0` directory as Parquet files. This completes the preprocessing and cleaning phase, and the resulting dataset is now ready for exploratory data analysis (EDA) and model building."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

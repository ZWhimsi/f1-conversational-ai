{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVdfICe1g1-n"
      },
      "source": [
        "# Mistral 7B F1 QA Evaluation\n",
        "\n",
        "This notebook evaluates Mistral 7B on the F1 QA dataset with multiple choice questions.\n",
        "\n",
        "It loads 500 JSON files (1500 QA pairs total), formats them as multiple choice questions, and evaluates the model's accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WQ2Ui0Dg1-p"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install transformers torch accelerate huggingface_hub tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "083lyuDwg1-r"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swec1zytg1-s"
      },
      "outputs": [],
      "source": [
        "# Import utilities\n",
        "import sys\n",
        "sys.path.append('/content')\n",
        "from f1_qa_utils import load_qa_dataset, format_multiple_choice, extract_answer_letter, extract_justification\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from pathlib import Path\n",
        "import time\n",
        "import gc\n",
        "from typing import List, Dict, Any\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lWJIqxug1-t"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "Set paths and model to evaluate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBUpofl8g1-t"
      },
      "outputs": [],
      "source": [
        "# Configuration for Mistral 7B\n",
        "MODEL_ID = \"mistralai/Mistral-7B-v0.1\"\n",
        "MODEL_NAME = \"mistral-7b\"\n",
        "\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Data_Collection_Code/f1_refined_data/f1_qa_outputs\"\n",
        "RESULTS_PATH = \"/content/drive/MyDrive/CS6220_Project/results\"\n",
        "\n",
        "# Create results directory if needed\n",
        "Path(RESULTS_PATH).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Configuration loaded:\")\n",
        "print(f\"   Model: {MODEL_NAME}\")\n",
        "print(f\"   Model ID: {MODEL_ID}\")\n",
        "print(f\"   Dataset: {DATASET_PATH}\")\n",
        "print(f\"   Results: {RESULTS_PATH}\")\n",
        "print(f\"   üìù Note: Model will be loaded directly from Hugging Face Hub\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cCoNnQmg1-t"
      },
      "source": [
        "## Load QA Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQCs98fYg1-t"
      },
      "outputs": [],
      "source": [
        "# Load all QA pairs\n",
        "qa_dataset = load_qa_dataset(DATASET_PATH)\n",
        "\n",
        "print(f\"üìä Dataset Statistics:\")\n",
        "print(f\"   Total QA pairs: {len(qa_dataset)}\")\n",
        "print(f\"   Expected: 1500 (500 files √ó 3 QA pairs each)\")\n",
        "\n",
        "# Show example\n",
        "if qa_dataset:\n",
        "    print(f\"\\nüìù Example QA pair:\")\n",
        "    example = qa_dataset[0]\n",
        "    print(f\"   Question: {example['question']}\")\n",
        "    print(f\"   Correct Answer: {example['correct_answer']}\")\n",
        "    print(f\"   Wrong Options: {example['wrong_options']}\")\n",
        "\n",
        "    # Show example of formatted prompt\n",
        "    print(f\"\\nüìù Example formatted prompt:\")\n",
        "    prompt, correct_letter = format_multiple_choice(\n",
        "        example['question'],\n",
        "        example['correct_answer'],\n",
        "        example['wrong_options']\n",
        "    )\n",
        "    print(f\"   Correct letter: {correct_letter}\")\n",
        "    print(f\"   Prompt preview: {prompt[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmXkXj0Ng1-u"
      },
      "source": [
        "## Model Evaluator Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIeYYC-eg1-u"
      },
      "outputs": [],
      "source": [
        "class F1QAEvaluator:\n",
        "    \"\"\"Evaluator for F1 QA models.\"\"\"\n",
        "\n",
        "    def __init__(self, model_id: str, model_name: str):\n",
        "        self.model_id = model_id\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load model and tokenizer directly from Hugging Face Hub.\"\"\"\n",
        "        try:\n",
        "            print(f\"üîÑ Loading {self.model_name} from {self.model_id}...\")\n",
        "\n",
        "            # Load tokenizer\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                self.model_id,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "\n",
        "            # Set padding token if not set\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            # Load model with updated parameter name\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_id,\n",
        "                trust_remote_code=True,\n",
        "                dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "                device_map=\"auto\" if self.device == \"cuda\" else \"cpu\"\n",
        "            )\n",
        "\n",
        "            print(f\"‚úÖ {self.model_name} loaded successfully on {self.device}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load {self.model_name}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def generate_response(self, prompt: str, max_new_tokens: int = 250) -> str:\n",
        "        \"\"\"Generate response from the model with proper error handling and attention mask.\"\"\"\n",
        "        try:\n",
        "            # Tokenize with proper attention mask handling\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                padding=True\n",
        "            )\n",
        "\n",
        "            if self.device == \"cuda\":\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Use different generation parameters based on model type\n",
        "                generation_kwargs = {\n",
        "                    \"input_ids\": inputs[\"input_ids\"],\n",
        "                    \"attention_mask\": inputs[\"attention_mask\"],\n",
        "                    \"max_new_tokens\": max_new_tokens,\n",
        "                    \"do_sample\": True,\n",
        "                    \"temperature\": 0.7,\n",
        "                    \"top_p\": 0.9,\n",
        "                    \"pad_token_id\": self.tokenizer.eos_token_id,\n",
        "                    \"eos_token_id\": self.tokenizer.eos_token_id,\n",
        "                }\n",
        "\n",
        "                # Add model-specific parameters\n",
        "                if \"phi\" in self.model_name.lower():\n",
        "                    # Phi-3 models work better with these settings\n",
        "                    generation_kwargs.update({\n",
        "                        \"use_cache\": False,  # Disable cache to avoid DynamicCache issues\n",
        "                        \"repetition_penalty\": 1.1\n",
        "                    })\n",
        "                elif \"falcon\" in self.model_name.lower():\n",
        "                    # Falcon models work better with these settings\n",
        "                    generation_kwargs.update({\n",
        "                        \"repetition_penalty\": 1.1,\n",
        "                        \"no_repeat_ngram_size\": 2\n",
        "                    })\n",
        "                else:\n",
        "                    # Default settings for other models\n",
        "                    generation_kwargs.update({\n",
        "                        \"repetition_penalty\": 1.1\n",
        "                    })\n",
        "\n",
        "                outputs = self.model.generate(**generation_kwargs)\n",
        "\n",
        "            # Check if outputs is valid\n",
        "            if outputs is None or len(outputs) == 0:\n",
        "                return \"\"\n",
        "\n",
        "            # Decode only the new tokens (excluding the input prompt)\n",
        "            input_length = inputs[\"input_ids\"].shape[1]\n",
        "            if len(outputs[0]) > input_length:\n",
        "                new_tokens = outputs[0][input_length:]\n",
        "                response = self.tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
        "            else:\n",
        "                # If no new tokens generated, return empty string\n",
        "                response = \"\"\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error generating response: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def evaluate_dataset(self, qa_dataset: List[Dict]) -> Dict:\n",
        "        \"\"\"Evaluate model on entire dataset.\"\"\"\n",
        "        results = {\n",
        "            'model_name': self.model_name,\n",
        "            'total_questions': len(qa_dataset),\n",
        "            'correct': 0,\n",
        "            'incorrect': 0,\n",
        "            'invalid': 0,\n",
        "            'accuracy': 0.0,\n",
        "            'details': []\n",
        "        }\n",
        "\n",
        "        print(f\"\\nüöÄ Evaluating {self.model_name} on {len(qa_dataset)} questions...\")\n",
        "\n",
        "        for i, qa_pair in enumerate(tqdm(qa_dataset, desc=f\"Evaluating {self.model_name}\")):\n",
        "            question = qa_pair['question']\n",
        "            correct_answer = qa_pair['correct_answer']\n",
        "            wrong_options = qa_pair['wrong_options']\n",
        "\n",
        "            # Format as multiple choice\n",
        "            prompt, correct_letter = format_multiple_choice(question, correct_answer, wrong_options)\n",
        "\n",
        "            # Generate response\n",
        "            response = self.generate_response(prompt)\n",
        "\n",
        "            # Extract answer letter and justification\n",
        "            predicted_letter = extract_answer_letter(response)\n",
        "            justification = extract_justification(response)\n",
        "\n",
        "            # Check correctness\n",
        "            is_correct = predicted_letter == correct_letter\n",
        "            is_invalid = predicted_letter == \"\"\n",
        "\n",
        "            if is_correct:\n",
        "                results['correct'] += 1\n",
        "            elif is_invalid:\n",
        "                results['invalid'] += 1\n",
        "            else:\n",
        "                results['incorrect'] += 1\n",
        "\n",
        "            # Store details\n",
        "            results['details'].append({\n",
        "                'question': question,\n",
        "                'correct_answer': correct_answer,\n",
        "                'correct_letter': correct_letter,\n",
        "                'predicted_letter': predicted_letter,\n",
        "                'is_correct': is_correct,\n",
        "                'is_invalid': is_invalid,\n",
        "                'response': response,\n",
        "                'justification': justification,\n",
        "                'prompt': prompt\n",
        "            })\n",
        "\n",
        "        # Calculate accuracy\n",
        "        valid_responses = results['total_questions'] - results['invalid']\n",
        "        if valid_responses > 0:\n",
        "            results['accuracy'] = results['correct'] / valid_responses\n",
        "\n",
        "        print(f\"‚úÖ {self.model_name} evaluation complete:\")\n",
        "        print(f\"   Correct: {results['correct']}\")\n",
        "        print(f\"   Incorrect: {results['incorrect']}\")\n",
        "        print(f\"   Invalid: {results['invalid']}\")\n",
        "        print(f\"   Accuracy: {results['accuracy']:.2%}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Clear model from memory.\"\"\"\n",
        "        del self.model\n",
        "        del self.tokenizer\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        print(f\"üßπ Cleaned up {self.model_name} from memory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OYADjP3g1-w"
      },
      "source": [
        "## Run Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlIG8_Psg1-w"
      },
      "outputs": [],
      "source": [
        "# Evaluate Mistral 7B\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"ü§ñ Evaluating Model: {MODEL_NAME}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "evaluator = F1QAEvaluator(MODEL_ID, MODEL_NAME)\n",
        "\n",
        "if evaluator.load_model():\n",
        "    results = evaluator.evaluate_dataset(qa_dataset)\n",
        "    evaluator.cleanup()\n",
        "\n",
        "    # Save results to JSON\n",
        "    output_file = Path(RESULTS_PATH) / f\"mistral_7b_evaluation_results_{int(time.time())}.json\"\n",
        "\n",
        "    results_summary = {\n",
        "        'metadata': {\n",
        "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'model_name': MODEL_NAME,\n",
        "            'model_id': MODEL_ID,\n",
        "            'total_qa_pairs': len(qa_dataset),\n",
        "            'device': evaluator.device\n",
        "        },\n",
        "        'results': results\n",
        "    }\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\nüíæ Results saved to: {output_file}\")\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìä MISTRAL 7B EVALUATION SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"   Accuracy: {results['accuracy']:.2%}\")\n",
        "    print(f\"   Correct: {results['correct']}/{results['total_questions']}\")\n",
        "    print(f\"   Incorrect: {results['incorrect']}\")\n",
        "    print(f\"   Invalid: {results['invalid']}\")\n",
        "    print(f\"\\nüíæ Full results saved to: {output_file}\")\n",
        "\n",
        "else:\n",
        "    print(f\"‚ùå Failed to load {MODEL_NAME}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

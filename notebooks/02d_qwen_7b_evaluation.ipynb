{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen 7B Chat F1 QA Evaluation\n",
        "\n",
        "This notebook evaluates Qwen 7B Chat on the F1 QA dataset with multiple choice questions.\n",
        "\n",
        "It loads 500 JSON files (1500 QA pairs total), formats them as multiple choice questions, and evaluates the model's accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages with latest versions\n",
        "%pip install transformers>=4.40.0 torch>=2.0.0 accelerate>=0.20.0 huggingface_hub>=0.20.0 tqdm transformers_stream_generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"✅ Google Drive mounted successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import utilities with latest transformers syntax\n",
        "import sys\n",
        "sys.path.append('/content')\n",
        "from f1_qa_utils import load_qa_dataset, format_multiple_choice, extract_answer_letter, extract_justification\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from pathlib import Path\n",
        "import time\n",
        "import gc\n",
        "from typing import List, Dict, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set paths and model to evaluate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for Qwen 7B Chat\n",
        "MODEL_ID = \"Qwen/Qwen-7B-Chat\"\n",
        "MODEL_NAME = \"qwen-7b-chat\"\n",
        "\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Data_Collection_Code/f1_refined_data/f1_qa_outputs\"\n",
        "RESULTS_PATH = \"/content/drive/MyDrive/CS6220_Project/results\"\n",
        "\n",
        "# Create results directory if needed\n",
        "Path(RESULTS_PATH).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"✅ Configuration loaded:\")\n",
        "print(f\"   Model: {MODEL_NAME}\")\n",
        "print(f\"   Model ID: {MODEL_ID}\")\n",
        "print(f\"   Dataset: {DATASET_PATH}\")\n",
        "print(f\"   Results: {RESULTS_PATH}\")\n",
        "print(f\"   📝 Note: Model will be loaded directly from Hugging Face Hub\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load QA Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all QA pairs\n",
        "qa_dataset = load_qa_dataset(DATASET_PATH)\n",
        "\n",
        "print(f\"📊 Dataset Statistics:\")\n",
        "print(f\"   Total QA pairs: {len(qa_dataset)}\")\n",
        "print(f\"   Expected: 1500 (500 files × 3 QA pairs each)\")\n",
        "\n",
        "# Show example\n",
        "if qa_dataset:\n",
        "    print(f\"\\n📝 Example QA pair:\")\n",
        "    example = qa_dataset[0]\n",
        "    print(f\"   Question: {example['question']}\")\n",
        "    print(f\"   Correct Answer: {example['correct_answer']}\")\n",
        "    print(f\"   Wrong Options: {example['wrong_options']}\")\n",
        "    \n",
        "    # Show example of formatted prompt\n",
        "    print(f\"\\n📝 Example formatted prompt:\")\n",
        "    prompt, correct_letter = format_multiple_choice(\n",
        "        example['question'], \n",
        "        example['correct_answer'], \n",
        "        example['wrong_options']\n",
        "    )\n",
        "    print(f\"   Correct letter: {correct_letter}\")\n",
        "    print(f\"   Prompt preview: {prompt[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluator Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class F1QAEvaluator:\n",
        "    \"\"\"Evaluator for F1 QA models with latest transformers syntax.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_id: str, model_name: str):\n",
        "        self.model_id = model_id\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    \n",
        "    def load_model(self):\n",
        "        \"\"\"Load model and tokenizer directly from Hugging Face Hub.\"\"\"\n",
        "        try:\n",
        "            print(f\"🔄 Loading {self.model_name} from {self.model_id}...\")\n",
        "            \n",
        "            # Load tokenizer with latest syntax\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                self.model_id,\n",
        "                trust_remote_code=True,\n",
        "                use_fast=True\n",
        "            )\n",
        "            \n",
        "            # Set padding token if not set\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            \n",
        "            # Load model with latest syntax and optimizations\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_id,\n",
        "                trust_remote_code=True,\n",
        "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "                device_map=\"auto\" if self.device == \"cuda\" else None,\n",
        "                attn_implementation=\"eager\"  # Use eager attention for better compatibility\n",
        "            )\n",
        "            \n",
        "            # Move to device if not using device_map\n",
        "            if self.device == \"cuda\" and not hasattr(self.model, 'hf_device_map'):\n",
        "                self.model = self.model.to(self.device)\n",
        "            \n",
        "            print(f\"✅ {self.model_name} loaded successfully on {self.device}\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to load {self.model_name}: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def generate_response(self, prompt: str, max_new_tokens: int = 250) -> str:\n",
        "        \"\"\"Generate response using latest transformers generation syntax.\"\"\"\n",
        "        try:\n",
        "            # Tokenize with proper attention mask handling\n",
        "            inputs = self.tokenizer(\n",
        "                prompt, \n",
        "                return_tensors=\"pt\", \n",
        "                truncation=True, \n",
        "                max_length=512,\n",
        "                padding=True\n",
        "            )\n",
        "            \n",
        "            if self.device == \"cuda\":\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "            \n",
        "            # Create generation config with latest syntax\n",
        "            generation_config = GenerationConfig(\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                repetition_penalty=1.1,\n",
        "                use_cache=True,  # Qwen works well with cache enabled\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=False\n",
        "            )\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs[\"input_ids\"],\n",
        "                    attention_mask=inputs[\"attention_mask\"],\n",
        "                    generation_config=generation_config\n",
        "                )\n",
        "            \n",
        "            # Check if outputs is valid\n",
        "            if outputs is None or not hasattr(outputs, 'sequences'):\n",
        "                return \"\"\n",
        "            \n",
        "            # Decode only the new tokens (excluding the input prompt)\n",
        "            input_length = inputs[\"input_ids\"].shape[1]\n",
        "            if len(outputs.sequences[0]) > input_length:\n",
        "                new_tokens = outputs.sequences[0][input_length:]\n",
        "                response = self.tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
        "            else:\n",
        "                # If no new tokens generated, return empty string\n",
        "                response = \"\"\n",
        "            \n",
        "            return response\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error generating response: {e}\")\n",
        "            return \"\"\n",
        "    \n",
        "    def evaluate_dataset(self, qa_dataset: List[Dict]) -> Dict:\n",
        "        \"\"\"Evaluate model on entire dataset.\"\"\"\n",
        "        results = {\n",
        "            'model_name': self.model_name,\n",
        "            'total_questions': len(qa_dataset),\n",
        "            'correct': 0,\n",
        "            'incorrect': 0,\n",
        "            'invalid': 0,\n",
        "            'accuracy': 0.0,\n",
        "            'details': []\n",
        "        }\n",
        "        \n",
        "        print(f\"\\n🚀 Evaluating {self.model_name} on {len(qa_dataset)} questions...\")\n",
        "        \n",
        "        for i, qa_pair in enumerate(tqdm(qa_dataset, desc=f\"Evaluating {self.model_name}\")):\n",
        "            question = qa_pair['question']\n",
        "            correct_answer = qa_pair['correct_answer']\n",
        "            wrong_options = qa_pair['wrong_options']\n",
        "            \n",
        "            # Format as multiple choice\n",
        "            prompt, correct_letter = format_multiple_choice(question, correct_answer, wrong_options)\n",
        "            \n",
        "            # Generate response\n",
        "            response = self.generate_response(prompt)\n",
        "            \n",
        "            # Extract answer letter and justification\n",
        "            predicted_letter = extract_answer_letter(response)\n",
        "            justification = extract_justification(response)\n",
        "            \n",
        "            # Check correctness\n",
        "            is_correct = predicted_letter == correct_letter\n",
        "            is_invalid = predicted_letter == \"\"\n",
        "            \n",
        "            if is_correct:\n",
        "                results['correct'] += 1\n",
        "            elif is_invalid:\n",
        "                results['invalid'] += 1\n",
        "            else:\n",
        "                results['incorrect'] += 1\n",
        "            \n",
        "            # Store details\n",
        "            results['details'].append({\n",
        "                'question': question,\n",
        "                'correct_answer': correct_answer,\n",
        "                'correct_letter': correct_letter,\n",
        "                'predicted_letter': predicted_letter,\n",
        "                'is_correct': is_correct,\n",
        "                'is_invalid': is_invalid,\n",
        "                'response': response,\n",
        "                'justification': justification,\n",
        "                'prompt': prompt\n",
        "            })\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        valid_responses = results['total_questions'] - results['invalid']\n",
        "        if valid_responses > 0:\n",
        "            results['accuracy'] = results['correct'] / valid_responses\n",
        "        \n",
        "        print(f\"✅ {self.model_name} evaluation complete:\")\n",
        "        print(f\"   Correct: {results['correct']}\")\n",
        "        print(f\"   Incorrect: {results['incorrect']}\")\n",
        "        print(f\"   Invalid: {results['invalid']}\")\n",
        "        print(f\"   Accuracy: {results['accuracy']:.2%}\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def cleanup(self):\n",
        "        \"\"\"Clear model from memory.\"\"\"\n",
        "        del self.model\n",
        "        del self.tokenizer\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        print(f\"🧹 Cleaned up {self.model_name} from memory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Qwen 7B Chat\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"🤖 Evaluating Model: {MODEL_NAME}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "evaluator = F1QAEvaluator(MODEL_ID, MODEL_NAME)\n",
        "\n",
        "if evaluator.load_model():\n",
        "    results = evaluator.evaluate_dataset(qa_dataset)\n",
        "    evaluator.cleanup()\n",
        "    \n",
        "    # Save results to JSON\n",
        "    output_file = Path(RESULTS_PATH) / f\"qwen_7b_chat_evaluation_results_{int(time.time())}.json\"\n",
        "    \n",
        "    results_summary = {\n",
        "        'metadata': {\n",
        "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'model_name': MODEL_NAME,\n",
        "            'model_id': MODEL_ID,\n",
        "            'total_qa_pairs': len(qa_dataset),\n",
        "            'device': evaluator.device\n",
        "        },\n",
        "        'results': results\n",
        "    }\n",
        "    \n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"\\n💾 Results saved to: {output_file}\")\n",
        "    \n",
        "    # Print summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"📊 QWEN 7B CHAT EVALUATION SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"   Accuracy: {results['accuracy']:.2%}\")\n",
        "    print(f\"   Correct: {results['correct']}/{results['total_questions']}\")\n",
        "    print(f\"   Incorrect: {results['incorrect']}\")\n",
        "    print(f\"   Invalid: {results['invalid']}\")\n",
        "    print(f\"\\n💾 Full results saved to: {output_file}\")\n",
        "    \n",
        "else:\n",
        "    print(f\"❌ Failed to load {MODEL_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
